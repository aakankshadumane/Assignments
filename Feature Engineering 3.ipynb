{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20f47849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62edbe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number\n",
      "0     123\n",
      "1     234\n",
      "2     644\n",
      "3      12\n",
      "4     899\n",
      "[[0.12514092]\n",
      " [0.25028185]\n",
      " [0.71251409]\n",
      " [0.        ]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "# application.\n",
    "Min-max scaling or Normalization is used in data preprocessing. For a dataset that contains features that\n",
    "vary in magnitudes or units, normalization is used to sacle the units in a range [0,1] or [-1,1]\n",
    "It is useful where outliers are not present and the distribution is unknown\n",
    "\n",
    "# Example:\n",
    "data = pd.DataFrame([123,234,644,12,899],columns=['Number'])\n",
    "print(data)\n",
    "    \n",
    "minmax = MinMaxScaler()\n",
    "scaled_data = minmax.fit_transform(data)\n",
    "print(scaled_data)\n",
    "# Data gets Scaled down to values ranging [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf156fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x    y\n",
      "0  3.0  4.0\n",
      "1  1.0  2.0\n",
      "2  6.0  8.0\n",
      "3  2.0  2.0\n",
      "   x_normalized  y_normalized\n",
      "0      0.600000      0.800000\n",
      "1      0.447214      0.894427\n",
      "2      0.600000      0.800000\n",
      "3      0.707107      0.707107\n"
     ]
    }
   ],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "# Provide an example to illustrate its application.\n",
    "Unit vector is used to scale the values of a feature to have a magnitude of 1 while preserving the direction \n",
    "of the original data.\n",
    "\n",
    "Unit Vector Scaling (Normalization):\n",
    "For each data point in a feature column, you calculate the magnitude of the vector formed by the values of \n",
    "that feature across all data points.\n",
    "Then, divide each value in the feature column by the magnitude obtained in the previous step.\n",
    "Unit Vector scaling normalizes the data points to have a magnitude of 1, which means they lie on the unit\n",
    "circle. It preserves the relative direction of data point\n",
    "\n",
    "Min-Max Scaling:\n",
    "Min-Max scaling scales the values of a feature to a specific range, typically between 0 and 1.\n",
    "For each data point in a feature column, you subtract the minimum value of the column from the data point and \n",
    "then divide it by the range (the difference between the maximum and minimum values of the column).\n",
    "Min-Max scaling doesn't guarantee a fixed direction between data points.\n",
    "\n",
    "# Creating dummy data\n",
    "data = {'x': [3.0, 1.0, 6.0, 2.0],\n",
    "        'y': [4.0, 2.0, 8.0, 2.0]}\n",
    "\n",
    "# Converted data to a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Calculate magnitude\n",
    "magnitude = np.sqrt(df['x'] ** 2 + df['y'] ** 2)\n",
    "\n",
    "# Divide each element by the magnitude\n",
    "df['x_normalized'] = df['x'] / magnitude\n",
    "df['y_normalized'] = df['y'] / magnitude\n",
    "\n",
    "print(df[['x_normalized', 'y_normalized']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c4537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7bc12e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -9.21874675],\n",
       "       [ -2.1762509 ],\n",
       "       [-16.26124259],\n",
       "       [ 22.78999529],\n",
       "       [  4.86624495]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "# example to illustrate its application.\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique.It transforms a high-dimensional \n",
    "dataset into a lower-dimensional one while preserving as much of the variance in the original data as possible.\n",
    "It is used to address problems such as the curse of dimensionality, reduce noise in data, and improve model\n",
    "performance by reducing the number of features without losing significant information.\n",
    "\n",
    "\n",
    "# Sample data (Height and Weight of individuals)\n",
    "data = np.array([[160, 60], [165, 65], [155, 55], [180, 85], [170, 70]])\n",
    "\n",
    "# Initialize PCA with one component (to reduce to one dimension)\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "# Fit PCA to the data and transform it to the lower-dimensional space\n",
    "data_reduced = pca.fit_transform(data)\n",
    "\n",
    "# data_reduced now contains the data in a one-dimensional space\n",
    "data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76237ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13103331,  0.12341098],\n",
       "       [-0.16273589,  0.06258618],\n",
       "       [ 0.38422562, -0.11300741],\n",
       "       [-0.32990894, -0.11644781],\n",
       "       [-0.0226141 ,  0.04345805]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "# Extraction? Provide an example to illustrate this concept.\n",
    "PCA Extracts new features as well as reduce dimensions\n",
    "PCA can be used for feature extraction: \n",
    "Data Transformation: PCA transforms the original data into a new set of features, which are linear combinations \n",
    "of the original features. These new features are called principal components.\n",
    "\n",
    "Ordering by Variance: PCA orders the principal components in descending order of the variance.\n",
    "\n",
    "Feature Selection: To perform feature extraction, you can select a subset of the top principal components\n",
    "that collectively explain a high percentage (e.g., 95%) of the total variance in the data.\n",
    " \n",
    "data = np.array([[0.7, 0.8, 0.6],\n",
    "                 [0.5, 0.7, 0.4],\n",
    "                 [0.9, 0.6, 0.8],\n",
    "                 [0.4, 0.5, 0.3],\n",
    "                 [0.6, 0.7, 0.5]])\n",
    "\n",
    "# Initialize PCA with two components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA to the data and transform it to the lower-dimensional space\n",
    "data_transformed = pca.fit_transform(data)\n",
    "data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "# contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "# preprocess the data.\n",
    "\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Define the features to be scaled\n",
    "features_to_scale = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Initialize Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to your data and transform it\n",
    "data[features_to_scale] = scaler.fit_transform(data[features_to_scale])\n",
    "\n",
    "By applying Min-Max scaling, all the features will have a consistent scale, making it easier \n",
    "to build a recommendation system that considers different feature values effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "# features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "# dimensionality of the dataset.\n",
    "\n",
    "The data first needs to be clean. Like there is no missing values, categorical values are encoded, etc\n",
    "Goal of PCA is to find a smaller set of uncorrelated variables (principal components) that capture most \n",
    "of the variance in the original data.\n",
    "PCA is sensitive to scale of features.So Satndard scaling is used first.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(your_data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of the variance\n",
    "pca_result = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bd28a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "# values to a range of -1 to 1.\n",
    "\n",
    "# Creating pandas Dataframe\n",
    "data = pd.DataFrame([1, 5, 10, 15, 20],columns=[\"Number\"])\n",
    "\n",
    "# Initialize scaler Instance with feature_range of -1 to 1\n",
    "scale = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scale.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1a506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "# Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "The number of principal components to retain in a PCA-based feature extraction depends on several factors:\n",
    "1. Variance : choice is to retain enough components to capture a high percentage of the total variance, \n",
    "    such as 95% or 99%.\n",
    "2. Requirements of your analysis or model : To Reduce dimensionality, fewer components can be reatined.\n",
    "3. Main patterns in the data should be remained to interpret the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146c121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b4f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4d8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7c44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
